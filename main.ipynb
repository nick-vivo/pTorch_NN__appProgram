{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "- Github-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ —É–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –∏–∑ —Å—Ä–µ–¥—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. –ö–æ–º–º–∏—Ç—ã —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ —Å–∞–π—Ç –Ω–µ –∑–∞—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.\n",
    "- –í—ã–ø–æ–ª–Ω–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–º–º–∏—Ç–æ–≤, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤–∞–º–∏ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã. –û–¥–∏–Ω –∫–æ–º–º–∏—Ç –±—É–¥–µ—Ç –ø—Ä–∏—Ä–∞–≤–Ω–∏–≤–∞—Ç—å—Å—è –∫ —Å–ø–∏—Å–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ.\n",
    "- –°–æ–æ–±—â–µ–Ω–∏—è –∫–æ–º–º–∏—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–µ –æ–ø–∏—Å—ã–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç.\n",
    "- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∫–æ–¥ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ —Ç–∞–∫, —á—Ç–æ –∫–∞–∂–¥–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–º–µ–µ—Ç –Ω–µ–±–æ–ª—å—à—É—é –∑–æ–Ω—É –æ—Ç–≤–µ—Ç—Å–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Ä–µ—à–∞–µ—Ç –æ–¥–Ω—É –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∑–∞–¥–∞—á—É, –∫–æ—Ç–æ—Ä—É—é –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –Ω–∏ –æ–¥–Ω–∞ –¥—Ä—É–≥–∞—è —Ñ—É–Ω–∫—Ü–∏—è.\n",
    "- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ñ–æ—Ä–º–∏—Ç—å –Ω–æ—É—Ç–±—É–∫ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Å–¥–∞—á–∏ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã.\n",
    "- –ö–æ–¥ –æ—Ñ–æ—Ä–º–∏—Ç—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å PEP8.\n",
    "- –§—É–Ω–∫—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å dosctring.\n",
    "- –°–∏–≥–Ω–∞—Ç—É—Ä–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º type-hinting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ `PyTorch`\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (TensorFlow, Keras, Caffe –∏ —Ç.–ø.) –≤ —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω–µ –¥–æ–ø—É—Å–∫–∞–µ—Ç—Å—è üò¢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤.\n",
    "2.   –ü—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é, —Ç–µ—Å—Ç–æ–≤—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ (–≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 80:10:10). –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã.\n",
    "3.   –ù–∞–ø–∏—Å–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "4.   –û–ø–∏—Å–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. **–í–ê–ñ–ù–û**: —á—Ç–æ —Ç–∞–∫ –∫–∞–∫ –≤–∞—à –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å —Ç–µ–∫—Å—Ç–æ–º, —Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–∏–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö (–ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤ —Ç—É—Ç–æ—Ä–∏–∞–ª–µ).\n",
    "4.   –ù–∞–ø–∏—Å–∞—Ç—å `train loop` (—Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è). –ü—Ä–æ–≤–µ—Å—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ –æ–±—É—á–µ–Ω–∏—é —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ `learning rate` (—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è) –∏ `batch size` (—Ä–∞–∑–º–µ—Ä –º–∏–Ω–∏-–ø–∞–∫–µ—Ç–∞). –í—ã–±—Ä–∞—Ç—å –ø–æ 3 –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è `learning rate` –∏ `batch size` (–∏—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –±—É–¥–µ—Ç 9).\n",
    "5.   –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤—ã–≤–µ—Å—Ç–∏ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (–æ—Å—å `x` - –∏—Ç–µ—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è/–Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏; –æ—Å—å `y` - –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å) –∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–æ—Å—å `x` - –∏—Ç–µ—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è/–Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏; –æ—Å—å `y` - –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞). –ì—Ä–∞—Ñ–∏–∫–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–≤–æ–¥–∏—Ç—å –∫–∞–∫ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π, —Ç–∞–∫ –∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "6.   –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ.\n",
    "7.   –°–¥–µ–ª–∞–π—Ç–µ –≤—ã–≤–æ–¥—ã –ø–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –ö–∞–∫—É—é –º–æ–¥–µ–ª—å –∏–∑ –≤—Å–µ—Ö –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?\n",
    "8.   –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.\n",
    "9.   –í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–æ–≤—Ç–æ—Ä–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –∏ –∑–∞–≥—Ä—É–∑–∫—É –≤–µ—Å–æ–≤.  –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–π—Ç–µ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ–µ –∫–∞–∫–æ–π-—Ç–æ –æ—Ç–∑—ã–≤/—Ä–µ—Ü–µ–Ω–∑–∏—é –∏ –≤—ã–≤–µ–¥–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–Ω–æ–≤–∞ —Å–ø–∞—Ä—Å–∏–º –Ω–∞—à –ª—é–±–∏–º—ã–π —Å–∞–π—Ç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from modules.Analysis_Data__appProgram.modules import annotation as ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_base = \"ann_base\"\n",
    "\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parrent_path = os.path.join(current_path, \"..\")\n",
    "\n",
    "if not os.path.exists(ann_base):\n",
    "    os.mkdir(ann_base)\n",
    "\n",
    "else:\n",
    "    shutil.rmtree(ann_base)\n",
    "    os.mkdir(ann_base)\n",
    "\n",
    "\n",
    "dataset_path = os.path.join(\"datasets\", \"dataset\")\n",
    "ann_path = os.path.join(ann_base, \"ann.csv\")\n",
    "\n",
    "df = ann.annotation_dataset(dataset_path, ann_path)\n",
    "df.drop(\"rel_path\", axis=1, inplace=True)\n",
    "\n",
    "df.rename(columns={\"class\": \"stars\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path: str) -> str:\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥–∞ –µ–≥–æ –≤ —Å—Ç—Ä–æ–∫—É\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "      –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "    –°—Ç—Ä–æ–∫–∞, –ø–æ–ª—É—á–µ–Ω–Ω–∞—è –∏–∑ –¥–∞–Ω–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"abs_path\"].apply(read_file)\n",
    "\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "df.drop_duplicates(subset=[\"text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_path</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>count_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>0</td>\n",
       "      <td>–ö–Ω–∏–≥–∞ –ø–æ–ª–Ω–∞ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–π.–ê–≤—Ç–æ—Ä –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç ...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>0</td>\n",
       "      <td>–ö–æ–≥–¥–∞ —è –ø–æ–≤–µ—Ä–Ω—É–ª–∞ –Ω–µ —Ç—É–¥–∞? –ü–æ—á–µ–º—É –Ω–∏–∫—Ç–æ –º–Ω–µ –Ω–µ...</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>0</td>\n",
       "      <td>–ü—Ä–µ—Ç–µ–Ω—Ü–∏–æ–∑–Ω–æ–µ –Ω–∏—á—Ç–æ, –∫–∞–∫ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –∫–Ω–∏–≥–∞ –≤ ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>0</td>\n",
       "      <td>–í—Ä–æ–¥–µ –±—ã –Ω–µ–ª—å–∑—è –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å —ç—Ç—É –∫–Ω–∏–≥—É –±—É–∫–≤–∞–ª—å...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>0</td>\n",
       "      <td>–ü–æ—Å—ã–ª —É –∫–Ω–∏–≥–∏ –≤–∞–∂–Ω—ã–π –∏ –Ω—É–∂–Ω—ã–π. –û—Ç—á–µ—Ç–ª–∏–≤–æ –ø–æ–Ω—è—Ç...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>4</td>\n",
       "      <td>–ö–Ω–∏–≥–∞ 100% –∑–∞—Å–ª—É–∂–∏–≤–∞–µ—Ç –≤–∞—à–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –≤—Ä–µ–º–µ...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>4</td>\n",
       "      <td>–ö–∞–∑–∞–ª–æ—Å—å –±—ã, –≤ –∫–Ω–∏–≥–µ –∏–∑–ª–æ–∂–µ–Ω—ã —Ç–µ –∏—Å—Ç–∏–Ω—ã, –∫–æ—Ç–æ—Ä...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>4</td>\n",
       "      <td>–ê–≤—Ç–æ—Ä –Ω–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤—ã–≤–∞–µ—Ç –∫ –º–æ–µ–º—É –±–æ–ª—å—à–æ–º—É —É–¥–æ–≤–æ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>4</td>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∏ –±–æ–≥–∞—Ç—ã–π —è–∑—ã–∫, —Ç–æ–Ω–∫–∏–π —é–º–æ—Ä, –∫–ª–∞—Å—Å–Ω—ã–µ...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>/home/nick/Desktop/Univ/Programming/end_5_lab/...</td>\n",
       "      <td>4</td>\n",
       "      <td>–û—á–µ–Ω—å —Ä–∞—Å—Å–ª–∞–±–ª—è—é—â–∞—è, –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–∞—è –∫–Ω–∏–≥–∞. –í–º–µ—Å—Ç–µ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1447 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abs_path  stars  \\\n",
       "0     /home/nick/Desktop/Univ/Programming/end_5_lab/...      0   \n",
       "1     /home/nick/Desktop/Univ/Programming/end_5_lab/...      0   \n",
       "2     /home/nick/Desktop/Univ/Programming/end_5_lab/...      0   \n",
       "4     /home/nick/Desktop/Univ/Programming/end_5_lab/...      0   \n",
       "5     /home/nick/Desktop/Univ/Programming/end_5_lab/...      0   \n",
       "...                                                 ...    ...   \n",
       "1500  /home/nick/Desktop/Univ/Programming/end_5_lab/...      4   \n",
       "1501  /home/nick/Desktop/Univ/Programming/end_5_lab/...      4   \n",
       "1502  /home/nick/Desktop/Univ/Programming/end_5_lab/...      4   \n",
       "1503  /home/nick/Desktop/Univ/Programming/end_5_lab/...      4   \n",
       "1504  /home/nick/Desktop/Univ/Programming/end_5_lab/...      4   \n",
       "\n",
       "                                                   text  count_words  \n",
       "0     –ö–Ω–∏–≥–∞ –ø–æ–ª–Ω–∞ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–π.–ê–≤—Ç–æ—Ä –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç ...           48  \n",
       "1     –ö–æ–≥–¥–∞ —è –ø–æ–≤–µ—Ä–Ω—É–ª–∞ –Ω–µ —Ç—É–¥–∞? –ü–æ—á–µ–º—É –Ω–∏–∫—Ç–æ –º–Ω–µ –Ω–µ...          118  \n",
       "2     –ü—Ä–µ—Ç–µ–Ω—Ü–∏–æ–∑–Ω–æ–µ –Ω–∏—á—Ç–æ, –∫–∞–∫ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –∫–Ω–∏–≥–∞ –≤ ...           22  \n",
       "4     –í—Ä–æ–¥–µ –±—ã –Ω–µ–ª—å–∑—è –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å —ç—Ç—É –∫–Ω–∏–≥—É –±—É–∫–≤–∞–ª—å...           30  \n",
       "5     –ü–æ—Å—ã–ª —É –∫–Ω–∏–≥–∏ –≤–∞–∂–Ω—ã–π –∏ –Ω—É–∂–Ω—ã–π. –û—Ç—á–µ—Ç–ª–∏–≤–æ –ø–æ–Ω—è—Ç...          126  \n",
       "...                                                 ...          ...  \n",
       "1500  –ö–Ω–∏–≥–∞ 100% –∑–∞—Å–ª—É–∂–∏–≤–∞–µ—Ç –≤–∞—à–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –≤—Ä–µ–º–µ...          119  \n",
       "1501  –ö–∞–∑–∞–ª–æ—Å—å –±—ã, –≤ –∫–Ω–∏–≥–µ –∏–∑–ª–æ–∂–µ–Ω—ã —Ç–µ –∏—Å—Ç–∏–Ω—ã, –∫–æ—Ç–æ—Ä...           40  \n",
       "1502  –ê–≤—Ç–æ—Ä –Ω–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤—ã–≤–∞–µ—Ç –∫ –º–æ–µ–º—É –±–æ–ª—å—à–æ–º—É —É–¥–æ–≤–æ...           33  \n",
       "1503  –∫—Ä–∞—Å–∏–≤—ã–π –∏ –±–æ–≥–∞—Ç—ã–π —è–∑—ã–∫, —Ç–æ–Ω–∫–∏–π —é–º–æ—Ä, –∫–ª–∞—Å—Å–Ω—ã–µ...          126  \n",
       "1504  –û—á–µ–Ω—å —Ä–∞—Å—Å–ª–∞–±–ª—è—é—â–∞—è, –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–∞—è –∫–Ω–∏–≥–∞. –í–º–µ—Å—Ç–µ...           52  \n",
       "\n",
       "[1447 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(string: str) -> int:\n",
    "    \"\"\"\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ\n",
    "    Parameters\n",
    "    ----------\n",
    "    string: str\n",
    "      –ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_string = re.sub(r\"[^\\w\\s]\", \" \", string)\n",
    "\n",
    "    words = cleaned_string.split()\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "\n",
    "df[\"count_words\"] = df[\"text\"].apply(count_words)\n",
    "df[\"stars\"] = df[\"stars\"].astype(int) - 1\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"sdk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_non_alphabets =lambda x: re.sub(r'[^–∞-—è–ê-–Ø]',' ',x)\n",
    "\n",
    "tokenize = lambda x: word_tokenize(x)\n",
    "\n",
    "ps = SnowballStemmer(\"russian\")\n",
    "stem = lambda w: [ ps.stem(x) for x in w ]\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "leammtizer = lambda w: [ morph.parse(x)[0].normal_form for x in w ]\n",
    "\n",
    "stop_words = set(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['—Å',\n",
       " '–¥—Ä—É–≥',\n",
       " '—Å—Ç–æ—Ä–æ–Ω',\n",
       " '–¥–∞–ª—å–Ω',\n",
       " '—Ä–∞–∑–≤–∏—Ç',\n",
       " '—Ä–∞–∑–ª–∏—á–Ω',\n",
       " '—Ñ–æ—Ä–º',\n",
       " '–¥–µ—è—Ç–µ–ª—å–Ω',\n",
       " '—Ç—Ä–µ–±',\n",
       " '–æ—Ç',\n",
       " '–Ω–∞—Å',\n",
       " '—Å–∏—Å—Ç–µ–º–Ω',\n",
       " '–∞–Ω–∞–ª–∏–∑',\n",
       " '–Ω–æ–≤',\n",
       " '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω',\n",
       " '—Å–æ–æ–±—Ä–∞–∂–µ–Ω',\n",
       " '–≤—ã—Å—à',\n",
       " '–ø–æ—Ä—è–¥–∫',\n",
       " '–∞',\n",
       " '—Ç–∞–∫–∂',\n",
       " '–Ω–∞—á–∞',\n",
       " '–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω',\n",
       " '—Ä–∞–±–æ—Ç',\n",
       " '–ø–æ',\n",
       " '—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω',\n",
       " '–ø–æ–∑–∏—Ü',\n",
       " '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è',\n",
       " '—Å–æ–±',\n",
       " '–∏–Ω—Ç–µ—Ä–µ—Å–Ω',\n",
       " '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç',\n",
       " '–ø—Ä–æ–≤–µ—Ä–∫',\n",
       " '–∏',\n",
       " '–¥–∞–ª—å–Ω',\n",
       " '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω',\n",
       " '—Ä–∞–∑–≤–∏—Ç',\n",
       " '–ø—Ä–æ–µ–∫—Ç',\n",
       " '–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω',\n",
       " '–ø—Ä–∞–∫—Ç–∏–∫',\n",
       " '–ø–æ–∫–∞–∑—ã–≤–∞',\n",
       " '—á—Ç–æ',\n",
       " '–¥–∞–ª—å–Ω',\n",
       " '—Ä–∞–∑–≤–∏—Ç',\n",
       " '—Ä–∞–∑–ª–∏—á–Ω',\n",
       " '—Ñ–æ—Ä–º',\n",
       " '–¥–µ—è—Ç–µ–ª—å–Ω',\n",
       " '–∏–≥—Ä–∞',\n",
       " '–≤–∞–∂–Ω',\n",
       " '—Ä–æ–ª',\n",
       " '–≤',\n",
       " '—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω',\n",
       " '–Ω–æ–≤',\n",
       " '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω',\n",
       " '–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫',\n",
       " '–æ–ø',\n",
       " '–ø–æ–∫–∞–∑—ã–≤–∞',\n",
       " '—á—Ç–æ',\n",
       " '–Ω–æ–≤',\n",
       " '–º–æ–¥–µ–ª',\n",
       " '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω',\n",
       " '–¥–µ—è—Ç–µ–ª—å–Ω',\n",
       " '–∏–≥—Ä–∞',\n",
       " '–≤–∞–∂–Ω',\n",
       " '—Ä–æ–ª',\n",
       " '–≤',\n",
       " '—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω',\n",
       " '–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω',\n",
       " '–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω',\n",
       " '—Ä–∞–∑–≤–∏—Ç',\n",
       " '–¥–æ—Ä–æ–≥',\n",
       " '–¥—Ä—É–∑',\n",
       " '–Ω–∞—á–∞',\n",
       " '–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω',\n",
       " '—Ä–∞–±–æ—Ç',\n",
       " '–ø–æ',\n",
       " '—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω',\n",
       " '–ø–æ–∑–∏—Ü',\n",
       " '–ø–æ–∑–≤–æ–ª—è',\n",
       " '–≤—ã–ø–æ–ª–Ω',\n",
       " '–≤–∞–∂–Ω',\n",
       " '–∑–∞–¥–∞–Ω',\n",
       " '–ø–æ',\n",
       " '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫',\n",
       " '—Å–∏—Å—Ç–µ–º',\n",
       " '–æ–±—É—á–µ–Ω',\n",
       " '–∫–∞–¥—Ä',\n",
       " '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤',\n",
       " '–Ω–∞—Å—É—â–Ω',\n",
       " '–ø–æ—Ç—Ä–µ–±–Ω',\n",
       " '–ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω',\n",
       " '–ø—Ä–∞–∫—Ç–∏–∫',\n",
       " '–ø–æ–∫–∞–∑—ã–≤–∞',\n",
       " '—á—Ç–æ',\n",
       " '—Ä–µ–∞–ª–∏–∑–∞—Ü',\n",
       " '–Ω–∞–º–µ—á–µ–Ω',\n",
       " '–ø–ª–∞–Ω',\n",
       " '—Ä–∞–∑–≤–∏—Ç',\n",
       " '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è',\n",
       " '—Å–æ–±',\n",
       " '–∏–Ω—Ç–µ—Ä–µ—Å–Ω',\n",
       " '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç',\n",
       " '–ø—Ä–æ–≤–µ—Ä–∫']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strka = \"\"\"99 –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–±—É–µ—Ç –æ—Ç –Ω–∞—Å —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π!\n",
    "\n",
    "–°–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã—Å—à–µ–≥–æ –ø–æ—Ä—è–¥–∫–∞, –∞ —Ç–∞–∫–∂–µ –Ω–∞—á–∞–ª–æ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø—Ä–æ–≤–µ—Ä–∫\n",
    "–∏ –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–æ–µ–∫—Ç–∞! –ü–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –≤ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –Ω–æ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π?\n",
    "\n",
    "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –≤ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π \n",
    "–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è! –î–æ—Ä–æ–≥–∏–µ –¥—Ä—É–∑—å—è, –Ω–∞—á–∞–ª–æ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–∑–∏—Ü–∏–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤–∞–∂–Ω–µ–π—à–∏–µ –∑–∞–¥–∞–Ω–∏—è –ø–æ \n",
    "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –Ω–∞—Å—É—â–Ω—ã–º –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º.\n",
    "\n",
    "–ü–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–º–µ—á–µ–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏...\"\"\"\n",
    "\n",
    "strka = remove_non_alphabets(strka)\n",
    "strka\n",
    "\n",
    "strka = tokenize(strka)\n",
    "strka\n",
    "\n",
    "strka = stem(strka)\n",
    "strka\n",
    "\n",
    "leammtizer(strka)\n",
    "strka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : [=====] : Completed"
     ]
    }
   ],
   "source": [
    "print('Processing : [=', end='')\n",
    "df['text'] = df['text'].apply(remove_non_alphabets)\n",
    "print('=', end='')\n",
    "df['text'] = df['text'].apply(tokenize) # [ word_tokenize(row) for row in df['text']]\n",
    "print('=', end='')\n",
    "df['text'] = df['text'].apply(stem)\n",
    "print('=', end='')\n",
    "df['text'] = df['text'].apply(leammtizer)\n",
    "print('=', end='')\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(x))\n",
    "print('] : Completed', end='')\n",
    "\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "# df.to_csv(\"sdd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>count_words</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–∫–Ω–∏–≥–∞ –ø–æ–ª–Ω–∞ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ—Ä –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä —Å–µ–± ...</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–∫–æ–≥–¥ —è –ø–æ–≤–µ—Ä–Ω—É—Ç—å –Ω–µ —Ç—É–¥ –ø–æ—á –Ω–∏–∫—Ç–∞ —è –Ω–µ —Å–∫–∞–∑ —á—Ç...</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ø—Ä–µ—Ç–µ–Ω—Ü–∏–æ–∑–Ω–∞ –Ω–∏—á—Ç –∫–∞–∫ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–Ω–∏–≥–∞ –≤ —ç—Ç ...</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤—Ä–æ–¥ –±—ã –Ω–µ–ª—å–∑ –≤–æ—Å–ø—Ä–∏–Ω–∏—Ç—å —ç—Ç –∫–Ω–∏–≥–∞ –±—É–∫–≤–∞–ª—å–Ω–∞ –Ω–æ...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>–ø–æ—Å —É –∫–Ω–∏–≥–∞ –≤–∞–∂–Ω –∏ –Ω—É–∂–Ω –æ—Ç—á—ë—Ç–ª–∏–≤—ã–π –ø–æ–Ω—è—Ç–Ω —á—Ç–æ ...</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>–∫–Ω–∏–≥–∞ –∑–∞—Å–ª—É–∂–∏–≤—ã–π –≤–∞—à –≤–Ω–∏–º–∞–Ω –∏ –≤—Ä–µ–º—è –Ω–∞—Å—Ç–æ–ª—å–∫ –æ...</td>\n",
       "      <td>119</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>–∫–∞–∑ –±—ã –≤ –∫–Ω–∏–≥–∞ –∏–∑–ª–æ–∂–∞ —Ç–æ—Ç –∏—Å—Ç–∏–Ω–∞ –∫–æ—Ç–æ—Ä –¥–∞–≤–Ω–∞ –∏...</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>–∞–≤—Ç–æ—Ä –Ω–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤—ã–≤ –∫ –º–æ –±–æ–ª—å—à–∞ —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ –æ...</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>–∫—Ä–∞—Å–∏–≤—ã–π –∏ –±–æ–≥–∞—Ç—ã–π —è–∑—ã–∫ —Ç–æ–Ω–∫ —é–º–æ—Ä –∫–ª–∞—Å—Å–Ω–∞ –ø–µ—Ä—Å...</td>\n",
       "      <td>126</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>–æ—á–Ω—ã–π —Ä–∞—Å—Å–ª–∞–±–ª—å –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–∞ –∫–Ω–∏–≥–∞ –≤–º–µ—Å—Ç–æ —Å –Ω–µ –ø...</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1447 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  count_words  stars\n",
       "0     –∫–Ω–∏–≥–∞ –ø–æ–ª–Ω–∞ —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ—Ä –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä —Å–µ–± ...           48      0\n",
       "1     –∫–æ–≥–¥ —è –ø–æ–≤–µ—Ä–Ω—É—Ç—å –Ω–µ —Ç—É–¥ –ø–æ—á –Ω–∏–∫—Ç–∞ —è –Ω–µ —Å–∫–∞–∑ —á—Ç...          118      0\n",
       "2     –ø—Ä–µ—Ç–µ–Ω—Ü–∏–æ–∑–Ω–∞ –Ω–∏—á—Ç –∫–∞–∫ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –∫–Ω–∏–≥–∞ –≤ —ç—Ç ...           22      0\n",
       "4     –≤—Ä–æ–¥ –±—ã –Ω–µ–ª—å–∑ –≤–æ—Å–ø—Ä–∏–Ω–∏—Ç—å —ç—Ç –∫–Ω–∏–≥–∞ –±—É–∫–≤–∞–ª—å–Ω–∞ –Ω–æ...           30      0\n",
       "5     –ø–æ—Å —É –∫–Ω–∏–≥–∞ –≤–∞–∂–Ω –∏ –Ω—É–∂–Ω –æ—Ç—á—ë—Ç–ª–∏–≤—ã–π –ø–æ–Ω—è—Ç–Ω —á—Ç–æ ...          126      0\n",
       "...                                                 ...          ...    ...\n",
       "1500  –∫–Ω–∏–≥–∞ –∑–∞—Å–ª—É–∂–∏–≤—ã–π –≤–∞—à –≤–Ω–∏–º–∞–Ω –∏ –≤—Ä–µ–º—è –Ω–∞—Å—Ç–æ–ª—å–∫ –æ...          119      4\n",
       "1501  –∫–∞–∑ –±—ã –≤ –∫–Ω–∏–≥–∞ –∏–∑–ª–æ–∂–∞ —Ç–æ—Ç –∏—Å—Ç–∏–Ω–∞ –∫–æ—Ç–æ—Ä –¥–∞–≤–Ω–∞ –∏...           40      4\n",
       "1502  –∞–≤—Ç–æ—Ä –Ω–µ —Ä–∞–∑–æ—á–∞—Ä–æ–≤—ã–≤ –∫ –º–æ –±–æ–ª—å—à–∞ —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ –æ...           33      4\n",
       "1503  –∫—Ä–∞—Å–∏–≤—ã–π –∏ –±–æ–≥–∞—Ç—ã–π —è–∑—ã–∫ —Ç–æ–Ω–∫ —é–º–æ—Ä –∫–ª–∞—Å—Å–Ω–∞ –ø–µ—Ä—Å...          126      4\n",
       "1504  –æ—á–Ω—ã–π —Ä–∞—Å—Å–ª–∞–±–ª—å –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω–∞ –∫–Ω–∏–≥–∞ –≤–º–µ—Å—Ç–æ —Å –Ω–µ –ø...           52      4\n",
       "\n",
       "[1447 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_file = df[['text', 'count_words', 'stars']]\n",
    "df_file\n",
    "# df_file.to_csv(\"file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abs_path       1447\n",
       "stars          1447\n",
       "text           1447\n",
       "count_words    1447\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stars: 1\n",
      "Max: 272\n",
      "Min: 0\n",
      "Mean: 101\n",
      "===========================================\n",
      "Stars: 2\n",
      "Max: 143\n",
      "Min: 8\n",
      "Mean: 104\n",
      "===========================================\n",
      "Stars: 3\n",
      "Max: 146\n",
      "Min: 3\n",
      "Mean: 109\n",
      "===========================================\n",
      "Stars: 4\n",
      "Max: 147\n",
      "Min: 9\n",
      "Mean: 110\n",
      "===========================================\n",
      "Stars: 5\n",
      "Max: 144\n",
      "Min: 5\n",
      "Mean: 100\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"stars\").count()\n",
    "\n",
    "for stars, mini_df in df.groupby(\"stars\"):\n",
    "    print(f\"Stars: {stars + 1}\")\n",
    "    print(f\"Max: {mini_df.count_words.max()}\")\n",
    "    print(f\"Min: {mini_df.count_words.min()}\")\n",
    "    print(f\"Mean: {int(mini_df.count_words.mean())}\")\n",
    "    print(\"===========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'count_words', 'stars']]\n",
    "df.to_csv(r\"file.gz\", compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1447, 14000)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 14000\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "\n",
    "cv = CountVectorizer(max_features=max_words, stop_words=list(stop_words))\n",
    "sparse_matrix = cv.fit_transform(df['text']).toarray()\n",
    "sparse_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(max_words, 7000)\n",
    "        self.linear2 = nn.Linear(7000, 3500)\n",
    "        self.linear3 = nn.Linear(3500, 500)\n",
    "        self.linear4 = nn.Linear(500, 50)\n",
    "        self.linear5 = nn.Linear(50, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = self.linear5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(sparse_matrix, np.array(df['stars']), train_size=0.8, random_state=42)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Variable(torch.from_numpy(x_train)).float()\n",
    "y_train = Variable(torch.from_numpy(y_train)).long()\n",
    "\n",
    "x_test = Variable(torch.from_numpy(x_test)).float()\n",
    "y_test = Variable(torch.from_numpy(y_test)).long()\n",
    "\n",
    "x_val = Variable(torch.from_numpy(x_val)).float()\n",
    "y_val = Variable(torch.from_numpy(y_val)).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ññ0 LR: 0.001, Batch Size: 32, Epoch: 1/10, Loss (Train/Val): 1.5609/1.5465, Accuracy (Train/Val): 0.2437/0.2621\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m---> 92\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m         loss_test \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     94\u001b[0m         running_loss_test \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_test\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[109], line 11\u001b[0m, in \u001b[0;36mSimpleModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "models_base = \"models_base\"\n",
    "current_path = os.getcwd()\n",
    "\n",
    "if not os.path.exists(models_base):\n",
    "    os.mkdir(models_base)\n",
    "\n",
    "else:\n",
    "    shutil.rmtree(models_base)\n",
    "    os.mkdir(models_base)\n",
    "\n",
    "\n",
    "model_list = []\n",
    "epochs = 10\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "        model = SimpleModel()\n",
    "        optimizer = torch.optim.Adam(params=model.parameters() , lr=lr)\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–æ–≤ —Å –Ω–æ–≤—ã–º batch_size\n",
    "        train_loader = torch.utils.data.DataLoader(list(zip(x_train, y_train)), batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        test_loader = torch.utils.data.DataLoader(list(zip(x_test, y_test)), batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(list(zip(x_val, y_val)), batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
    "        model.train()\n",
    "        loss_values_train = []\n",
    "        acc_values_train = []\n",
    "        loss_values_val = []\n",
    "        acc_values_val = []\n",
    "        loss_values_test = []\n",
    "        acc_values_test = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "            average_loss_train = running_loss / len(train_loader)\n",
    "            accuracy_train = correct_train / len(x_train)\n",
    "            loss_values_train.append(average_loss_train)\n",
    "            acc_values_train.append(accuracy_train)\n",
    "\n",
    "            model_list.append(model)\n",
    "            \n",
    "            # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "            model.eval()\n",
    "            running_loss_val = 0.0\n",
    "            correct_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss_val = criterion(outputs, labels)\n",
    "                    running_loss_val += loss_val.item()\n",
    "                    _, predicted_val = torch.max(outputs.data, 1)\n",
    "                    correct_val += (predicted_val == labels).sum().item()\n",
    "\n",
    "            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "            average_loss_val = running_loss_val / len(val_loader)\n",
    "            accuracy_val = correct_val / len(x_val)\n",
    "            loss_values_val.append(average_loss_val)\n",
    "            acc_values_val.append(accuracy_val)\n",
    "\n",
    "            \n",
    "            # –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏\n",
    "            running_loss_test = 0.0\n",
    "            correct_test = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss_test = criterion(outputs, labels)\n",
    "                    running_loss_test += loss_test.item()\n",
    "                    _, predicted_test = torch.max(outputs.data, 1)\n",
    "                    correct_test += (predicted_test == labels).sum().item()\n",
    "\n",
    "            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "            average_loss_test = running_loss_test / len(test_loader)\n",
    "            accuracy_test = correct_test / len(x_test)\n",
    "            loss_values_test.append(average_loss_test)\n",
    "            acc_values_test.append(accuracy_test)\n",
    "\n",
    "\n",
    "            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É\n",
    "            print('‚Ññ{} LR: {}, Batch Size: {}, Epoch: {}/{}, Loss (Train/Val): {:.4f}/{:.4f}, Accuracy (Train/Val): {:.4f}/{:.4f}'.format(\n",
    "                index, lr, batch_size, epoch + 1, epochs, average_loss_train, average_loss_val, accuracy_train, accuracy_val))\n",
    "            index += 1\n",
    "            model_list.append(model)\n",
    "            # torch.save(model.state_dict(),os.path.join(models_base, f'model_{index}_lr_{lr}_bs_{batch_size}_epoch_{epoch + 1}.pth'))\n",
    "\n",
    "        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        # –ì—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(epochs), loss_values_train, label='Train')\n",
    "        plt.plot(range(epochs), loss_values_val, label='Validation')\n",
    "        plt.plot(range(epochs), loss_values_test, label='Test')\n",
    "        plt.title(f'Loss: lr: {lr} bs:{batch_size}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "\n",
    "        # –ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(epochs), acc_values_train, label='Train')\n",
    "        plt.plot(range(epochs), acc_values_val, label='Validation')\n",
    "        plt.plot(range(epochs), acc_values_test, label='Test')\n",
    "        plt.title(f'Accuracy: lr:{lr} bs:{batch_size}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –∫–∞–∫–∏—Ö-–Ω–∏–±—É–¥—å –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –∑–∞–∫–æ–Ω—á–µ–Ω–∞, –æ–Ω–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ model_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review = \"\"\"–ö–∞–∫ –∂–µ —è –¥–æ–ª–≥–æ —á–∏—Ç–∞–ª–∞ –∫–Ω–∏–≥—É, –Ω–æ –º–Ω–µ –ø—Ä–æ—Å—Ç–æ –Ω–µ —Ö–æ—á–µ—Ç—Å—è —Ä–∞—Å—Å—Ç–∞–≤–∞—Ç—å—Å—è —Å –∞–≤—Ç–æ—Ä–æ–º, —á–∏—Ç–∞–ª–∞ –∫–Ω–∏–≥—É –ø–æ —á—É—Ç—å-—á—É—Ç—å, –Ω–∞—Å–ª–∞–∂–¥–∞—è—Å—å –∞–≤—Ç–æ—Ä—Å–∫–æ–π –∑–∞–¥—É–º–∫–æ–π –∏ –Ω–µ–ø–æ–≤—Ç–æ—Ä–∏–º—ã–º —Å—Ç–∏–ª–µ–º. –ú–Ω–µ –≥–æ—Ä–µ—Å—Ç–Ω–æ –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ –≤—Å–µ –∫–Ω–∏–≥–∏ –∞–≤—Ç–æ—Ä–∞ —É–∂–µ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã, –æ—á–µ–Ω—å –±—É–¥—É –∂–¥–∞—Ç—å –Ω–æ–≤—ã—Ö –∏—Å—Ç–æ—Ä–∏–π. –í —ç—Ç–æ–π –∫–Ω–∏–≥–µ –º—ã –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ —É–∑–Ω–∞–µ–º –æ–± –º–∏—Ä–µ –û—Ä–¥—ç–Ω–µ, –æ–± –∏—Ö –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö. –ú–∞—Ä–≥–∞—Ä–∏—Ç–∞ –∂–∏–≤–µ—Ç –≤ –¥–≤–µ —Ç—ã—Å—è—á–∏ –≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç–æ–º –≥–æ–¥—É, –Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–µ–º—É —É–≤–ª–µ—á–µ–Ω–∏—é –∫—É–∫–ª–∞–º–∏ –∏ —Å—Ç—Ä–∞–Ω–Ω–æ–º—É –ø—Ä–æ–¥–∞–≤—Ü—É –¥–µ–≤—É—à–∫–∞ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –≤ –ø—Ä–æ—à–ª–æ–º –∏ –µ–π –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Å—Ç–∞—Ç—å –≥—É–≤–µ—Ä–Ω–∞–Ω—Ç–∫–æ–π –¥–µ–≤–æ—á–∫–∏ –ù–∞–¥–∏ –∏ –ø–æ–º–æ—á—å –µ–π –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç —Å—Ç—Ä–∞–Ω–Ω—ã—Ö –º–æ–Ω—Å—Ç—Ä–æ–≤, –∞ –µ—â–µ —Ä–∞–∑–≥–∞–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–∫—Ä—É—á–µ–Ω–Ω—ã—Ö —Å–µ–º–µ–π–Ω—ã—Ö —Ç–∞–π–Ω. –ö–æ–≥–¥–∞ –≤ —Ç–µ–∫—Å—Ç–µ –ø–æ—è–≤–∏–ª–∏—Å—å —á–∞—Å—ã, –≥–¥–µ –ª–∞—Å—Ç–æ—á–∫–∞ –ø–æ–¥ –¥—Ä—É–≥–∏–º —É–≥–ª–æ–º —É–∫–ª–æ–Ω–∞ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ –≤–æ—Ä–æ–Ω–∞, —É –º–µ–Ω—è —Å—Ä–∞–∑—É –≤–æ–∑–Ω–∏–∫–ª–∏ –ø–æ–¥–æ–∑—Ä–µ–Ω–∏—è, —á—Ç–æ –Ø—à–∞ ‚Äî —ç—Ç–æ —Ç–æ—Ç —Å–∞–º—ã–π –Ø–∫–æ–≤ –∏–∑ –∫–Ω–∏–≥–∏ –°–µ—Ä–¥—Ü–µ‚Ä¶\n",
    "\"\"\"\n",
    "new_vectorize = cv.transform([new_review]).toarray()\n",
    "new_review_tensor = torch.from_numpy(new_vectorize).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14000)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vectorize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ –æ—Ç–∑—ã–≤–∞: 1\n"
     ]
    }
   ],
   "source": [
    "model_list[index].eval()\n",
    "with torch.no_grad():\n",
    "    output = model_list[index](new_review_tensor)\n",
    "    _, predicted_label = torch.max(output.data, 1)\n",
    "\n",
    "print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ –æ—Ç–∑—ã–≤–∞: {predicted_label.item() + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_base = \"models_base\"\n",
    "current_path = os.getcwd()\n",
    "\n",
    "if not os.path.exists(models_base):\n",
    "    os.mkdir(models_base)\n",
    "\n",
    "else:\n",
    "    shutil.rmtree(models_base)\n",
    "    os.mkdir(models_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_list[index].state_dict(), os.path.join(models_base, f'model_{index}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
